{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data file\n",
    "df = pd.read_csv(\"NytApiData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop problem rows\n",
    "df = df.drop([18,33])\n",
    "df.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulls page based on URL\n",
    "def GetPage(Url):\n",
    "    page = requests.get(Url).text\n",
    "    soup = BeautifulSoup(page)\n",
    "    return soup\n",
    "\n",
    "#function for web scraper \n",
    "def GetText(soup):\n",
    "    # Get text from all <p> tags.\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Get the text from each of the “p” tags and strip surrounding whitespace.\n",
    "    p_tags_text = [tag.get_text().strip() for tag in p_tags]\n",
    "    # Get text from all <p> tags.\n",
    "    p_tags = soup.find_all('p')\n",
    "    # Get the text from each of the “p” tags and strip surrounding whitespace.\n",
    "    p_tags_text = [tag.get_text().strip() for tag in p_tags]\n",
    "    # Filter out sentences that contain newline characters '\\n' or don't contain periods.\n",
    "    sentence_list = [sentence for sentence in p_tags_text if not '\\n' in sentence]\n",
    "    sentence_list = [sentence for sentence in sentence_list if '.' in sentence]\n",
    "    # Combine list items into string.\n",
    "    article = ' '.join(sentence_list)\n",
    "    return article\n",
    "\n",
    "#funtion to summarize scraped data\n",
    "Sum = lambda x : summarize(x, ratio=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calls, formats, and returns scraped data into a dictonary\n",
    "def GetArticalSummarys(df):\n",
    "    ret = []\n",
    "    i = 0\n",
    "    for index, row in df.iterrows():\n",
    "        temp = {}\n",
    "        temp['Date'] = df.loc[index,\"Date\"]\n",
    "        url = df.loc[index,\"URL\"]\n",
    "        soup = GetPage(url)\n",
    "        article = GetText(soup)\n",
    "        if len(article) > 0:\n",
    "            temp['KeyWords'] = keywords(article)\n",
    "            temp['Artical'] = article\n",
    "            temp['good'] = 'yes'\n",
    "        else:\n",
    "            temp['KeyWords'] = \"None\"\n",
    "            temp['Artical'] = \"None\"\n",
    "        \n",
    "        temp['Source'] = df.loc[index,\"Source\"]\n",
    "        temp['News Desk'] = df.loc[index,\"News Desk\"]\n",
    "        temp['Headline'] = df.loc[index,\"Headline\"]\n",
    "        temp['Abstract'] = df.loc[index,\"Abstract\"]\n",
    "        temp['Subjects'] = df.loc[index,\"Subjects\"]\n",
    "        ret.append(temp)\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = GetArticalSummarys(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn dict intp dataframe\n",
    "outdf = pd.DataFrame.from_dict(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop bad rows\n",
    "outdf.drop(outdf.index[outdf['good'] != 'yes'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save data\n",
    "outdf.to_csv('ArticalData.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
